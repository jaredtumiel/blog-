<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary" /><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Friston’s Free Energy Principle Explained | jared tumiel</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="Friston’s Free Energy Principle Explained" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="The goal of this series of posts is to provide a friendly but rigorous guide to some of the key ideas underlying Karl Friston’s Free Energy Principle (henceforth: FEP). I will provide as much background as possible, and sprinkle in the intuition-pumps I find most helpful. I want this to be a technical guide, so we won’t shy away from any math1. Your reward for making it to the end will be a deep understanding of the core pieces of the Free Energy principle, the ability to explain it to an interested friend in plain english, and the ability to implement a simulation of Active Inference under the Free Energy Principle in Python! But before we get to the Python, we need to lay the groundwork… The good news is that the Jeremy Howard rule is holds: even if all of the math seems difficult, it will look much simpler in code &#8617;" />
<meta property="og:description" content="The goal of this series of posts is to provide a friendly but rigorous guide to some of the key ideas underlying Karl Friston’s Free Energy Principle (henceforth: FEP). I will provide as much background as possible, and sprinkle in the intuition-pumps I find most helpful. I want this to be a technical guide, so we won’t shy away from any math1. Your reward for making it to the end will be a deep understanding of the core pieces of the Free Energy principle, the ability to explain it to an interested friend in plain english, and the ability to implement a simulation of Active Inference under the Free Energy Principle in Python! But before we get to the Python, we need to lay the groundwork… The good news is that the Jeremy Howard rule is holds: even if all of the math seems difficult, it will look much simpler in code &#8617;" />
<link rel="canonical" href="https://jaredtumiel.github.io/blog/2020/08/08/free-energy1.html" />
<meta property="og:url" content="https://jaredtumiel.github.io/blog/2020/08/08/free-energy1.html" />
<meta property="og:site_name" content="jared tumiel" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-08-08T00:00:00-05:00" />
<script type="application/ld+json">
{"description":"The goal of this series of posts is to provide a friendly but rigorous guide to some of the key ideas underlying Karl Friston’s Free Energy Principle (henceforth: FEP). I will provide as much background as possible, and sprinkle in the intuition-pumps I find most helpful. I want this to be a technical guide, so we won’t shy away from any math1. Your reward for making it to the end will be a deep understanding of the core pieces of the Free Energy principle, the ability to explain it to an interested friend in plain english, and the ability to implement a simulation of Active Inference under the Free Energy Principle in Python! But before we get to the Python, we need to lay the groundwork… The good news is that the Jeremy Howard rule is holds: even if all of the math seems difficult, it will look much simpler in code &#8617;","@type":"BlogPosting","headline":"Friston’s Free Energy Principle Explained","dateModified":"2020-08-08T00:00:00-05:00","datePublished":"2020-08-08T00:00:00-05:00","url":"https://jaredtumiel.github.io/blog/2020/08/08/free-energy1.html","mainEntityOfPage":{"@type":"WebPage","@id":"https://jaredtumiel.github.io/blog/2020/08/08/free-energy1.html"},"@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/blog/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://jaredtumiel.github.io/blog/feed.xml" title="jared tumiel" /><!-- the google_analytics_id gets auto inserted from the config file -->



<script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,'script','//www.google-analytics.com/analytics.js','ga');ga('create','UA-145017725-1','auto');ga('require','displayfeatures');ga('send','pageview');</script>

<link rel="shortcut icon" type="image/x-icon" href="/blog/images/favicon.ico"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Friston’s Free Energy Principle Explained | jared tumiel</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="Friston’s Free Energy Principle Explained" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="The goal of this series of posts is to provide a friendly but rigorous guide to some of the key ideas underlying Karl Friston’s Free Energy Principle (henceforth: FEP). I will provide as much background as possible, and sprinkle in the intuition-pumps I find most helpful. I want this to be a technical guide, so we won’t shy away from any math1. Your reward for making it to the end will be a deep understanding of the core pieces of the Free Energy principle, the ability to explain it to an interested friend in plain english, and the ability to implement a simulation of Active Inference under the Free Energy Principle in Python! But before we get to the Python, we need to lay the groundwork… The good news is that the Jeremy Howard rule is holds: even if all of the math seems difficult, it will look much simpler in code &#8617;" />
<meta property="og:description" content="The goal of this series of posts is to provide a friendly but rigorous guide to some of the key ideas underlying Karl Friston’s Free Energy Principle (henceforth: FEP). I will provide as much background as possible, and sprinkle in the intuition-pumps I find most helpful. I want this to be a technical guide, so we won’t shy away from any math1. Your reward for making it to the end will be a deep understanding of the core pieces of the Free Energy principle, the ability to explain it to an interested friend in plain english, and the ability to implement a simulation of Active Inference under the Free Energy Principle in Python! But before we get to the Python, we need to lay the groundwork… The good news is that the Jeremy Howard rule is holds: even if all of the math seems difficult, it will look much simpler in code &#8617;" />
<link rel="canonical" href="https://jaredtumiel.github.io/blog/2020/08/08/free-energy1.html" />
<meta property="og:url" content="https://jaredtumiel.github.io/blog/2020/08/08/free-energy1.html" />
<meta property="og:site_name" content="jared tumiel" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-08-08T00:00:00-05:00" />
<script type="application/ld+json">
{"description":"The goal of this series of posts is to provide a friendly but rigorous guide to some of the key ideas underlying Karl Friston’s Free Energy Principle (henceforth: FEP). I will provide as much background as possible, and sprinkle in the intuition-pumps I find most helpful. I want this to be a technical guide, so we won’t shy away from any math1. Your reward for making it to the end will be a deep understanding of the core pieces of the Free Energy principle, the ability to explain it to an interested friend in plain english, and the ability to implement a simulation of Active Inference under the Free Energy Principle in Python! But before we get to the Python, we need to lay the groundwork… The good news is that the Jeremy Howard rule is holds: even if all of the math seems difficult, it will look much simpler in code &#8617;","@type":"BlogPosting","headline":"Friston’s Free Energy Principle Explained","dateModified":"2020-08-08T00:00:00-05:00","datePublished":"2020-08-08T00:00:00-05:00","url":"https://jaredtumiel.github.io/blog/2020/08/08/free-energy1.html","mainEntityOfPage":{"@type":"WebPage","@id":"https://jaredtumiel.github.io/blog/2020/08/08/free-energy1.html"},"@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

<link href="https://unpkg.com/@primer/css/dist/primer.css" rel="stylesheet" />
<link rel="stylesheet" href="//use.fontawesome.com/releases/v5.0.7/css/all.css"><link type="application/atom+xml" rel="alternate" href="https://jaredtumiel.github.io/blog/feed.xml" title="jared tumiel" /><!-- the google_analytics_id gets auto inserted from the config file -->



<script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,'script','//www.google-analytics.com/analytics.js','ga');ga('create','UA-145017725-1','auto');ga('require','displayfeatures');ga('send','pageview');</script>


    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"> </script>
    <script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/blog/">jared tumiel</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/blog/about/">About Me</a><a class="page-link" href="/blog/search/">Search</a><a class="page-link" href="/blog/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Friston&#39;s Free Energy Principle Explained</h1><p class="post-meta post-meta-title"><time class="dt-published" datetime="2020-08-08T00:00:00-05:00" itemprop="datePublished">
        Aug 8, 2020
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      19 min read
    
</span></p>

    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <p>The goal of this series of posts is to provide a friendly but rigorous guide to some of the key ideas underlying Karl Friston’s Free Energy Principle (henceforth: FEP). I will provide as much background as possible, and sprinkle in the intuition-pumps I find most helpful. I want this to be a technical guide, so we won’t shy away from any math<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote">1</a></sup>. Your reward for making it to the end will be a deep understanding of the core pieces of the Free Energy principle, the ability to explain it to an interested friend in plain english, and the ability to implement a simulation of Active Inference under the Free Energy Principle in Python! But before we get to the Python, we need to lay the groundwork…</p>

<h2 id="covered-in-this-post">Covered in this post:</h2>

<ul>
  <li>Bayesian inference</li>
  <li>Phenotype as set of viable states</li>
  <li>Entropy and expected surprise</li>
  <li>Kullback-Liebler Divergence</li>
  <li>Recognition- and Generative-densities</li>
  <li>Derivation of ‘free energy’ term</li>
</ul>

<h2 id="introduction">Introduction</h2>

<p>Karl Friston’s Free Energy Principle has fascinated and baffled me since I first heard about it in a SlateStarCodex blog post<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote">2</a></sup>. Ever since, I’ve spent a good chunk of my spare time trying to understand the ideas and context that underpin the theory. Friston’s work is notoriously difficult to understand, something which Friston himself (and definitely the people who read his work) acknowledge with a wry shrug<sup id="fnref:3" role="doc-noteref"><a href="#fn:3" class="footnote">3</a></sup>. This comes down to a few things:</p>

<ul>
  <li>the maths itself is fairly daunting (although - I hope to show - not impenetrable, and well worth understanding), and Friston’s notation can be opaque until you get used to it.</li>
  <li>the fields that Friston draws on to derive the FEP are diverse and any given person is unlikely to have studied them all<sup id="fnref:4" role="doc-noteref"><a href="#fn:4" class="footnote">4</a></sup>. I happen to think this is what’s most exciting about the FEP, because it provides an excuse to learn more about:
    <ul>
      <li>Statistical Mechanics, especially non-equilibrium stat-mech</li>
      <li>Reinforcement Learning</li>
      <li>Neuroscience and Predictive Coding</li>
      <li>Dynamic systems and (stochastic) differential equations</li>
      <li>Information theory</li>
      <li>Variational methods</li>
      <li>Path-integral formulations and the Principle of Least Action</li>
      <li>Embodied cognition</li>
      <li>Bayesian inference, action under uncertainty</li>
      <li>Clinical psychiatry and computational corelates of pyschopathology</li>
    </ul>
  </li>
</ul>

<p>To begin, let’s look at some motivating ideas, which we’ll keep coming back to, with more and more formalism to back them up.</p>

<h2 id="you-a-thing-in-the-world">You: a thing in the world</h2>

<p>You are a thing<sup id="fnref:5" role="doc-noteref"><a href="#fn:5" class="footnote">5</a></sup> and you live in a world which you do not fully understand (or at-least, it should feel that way, if you’re paying any attention).</p>

<p>You get a constant stream of new information through your senses, and your brain somehow needs to use this information-deluge to do things like make exceptional scrambled eggs, renew your driver’s licence, floss your teeth, and other crucial survival-skills.</p>

<p><strong>Our first big insight into the FEP is that if you want to keep doing the survival thing, you should care not about the sense data itself, but about the stuff out there that causes it.</strong> The sense data is just receptors firing more or less often. What matters is that you have receptors that reliably fire in a pattern that tells you useful things like:</p>

<ul>
  <li>there’s a tiger there and she looks pissed</li>
  <li>it smells like chocolate cookies have just come out the oven, so you might get fed soon</li>
  <li>the molten chocolate cookie you just inhaled is scalding your oesophagus and causing long-term scarring</li>
</ul>

<p>So your sense-data isn’t random - it has external causes - but it’s not perfectly reliable either. It’s a noisy connection, and ambiguity abounds:</p>

<ul>
  <li>is that a burglar standing in the corner of your dark room, or did someone leave a coat hanging on your hatstand?</li>
  <li>is that the rustling of the wind in the leaves, or another, even scarier tiger stalking you?</li>
  <li>is the pressure of your ‘deep-tissue sports-massage’ a welcome relief from stiffness, or categorical proof that your masseuse is a sadist?</li>
</ul>

<p>In the time you read all of the above, you breathed several times, each of your cells used some ATP, some cells died or were phagocytosed, the state of your brain changed and a bunch of different neurons fired, and yet You are still a thing in the world. From this I infer that you did not suddenly dissolve into an unremarkable puddle of goo in the preceding 30 seconds. <strong>This is our second big insight into the FEP: as a system, many little things can change (you are dynamic), but you must keep yourself tightly bound into a larger pattern</strong>. Friston often refers to this as possessing an ‘attracting set’ - a set of states that all of your bizarre chemical processes can wiggle around in and between, but not out of.</p>

<p>There are lots of ways you could configure all of the atoms that currently constitute ‘you’. Unfortunately, the vast majority are unworkable, probably because the most likely arrangement of those atoms is spread in a thin mist somewhere between here and Neptune. The small subset of all possible ways to configure yourself that keeps you ‘you’ is your phenotype, and homeostasis is basically the process of keeping yourself in those states through feedback and self-regulation. To survive long-term, you need to have a high probability of occupying those states that are compatible with life (and a low probability of becoming a thin mist somewhere between here and Neptune)</p>

<p>Let’s make a concrete example out of temperature</p>

<h2 id="its-getting-hot-in-here-and-thats-surprising">It’s getting hot in here (and that’s surprising)</h2>

<p>Your body keeps itself near 37 degrees (Celsius), all the time<sup id="fnref:6" role="doc-noteref"><a href="#fn:6" class="footnote">6</a></sup></p>

<p>If you were suddenly to sense a temperature of 800 degrees, that would be surprising. And bad. As an organism, 800 degrees is not compatible with your continued existence, and is not something your phenotype is bequipped to deal with. In this sense, your body is implicitly making a claim that there is a low probability of you experiencing 800 degree heat, because if that were not the case, you would have a different body. Since 37 degrees seems cosy enough, your body <em>itself</em> is an implicit expectation of a high probability of sensing that temperature<sup id="fnref:7" role="doc-noteref"><a href="#fn:7" class="footnote">7</a></sup>.</p>

<p>So your evolved biology ‘expects’ 37 degrees , and when it doesn’t get it it’s surprised. This ‘<em>surprisal</em>’ is actually a formal term from information theory, where we quantify the amount of surprise as the negative natural logarithm of the probability of the observed outcome:</p>

<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ln</mi><mo>⁡</mo><mi>P</mi><mo stretchy="false">(</mo><mi>X</mi><mo>=</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\ln P(X = x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mop">ln</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.07847em;">X</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">x</span><span class="mclose">)</span></span></span></span></span>

<p>All this is saying is that the lower the probability we expect for an event, the more surprised we should be if we do in-fact observe that event, and conversely, if we think there is a high probability of an event happening, we should not be very surprised to see it occur. If you enter the lottery, and I tell you you’ve won, you’d be really really surprised, because you know the probability of that is small. If I tell you you’ve lost, you’re not really surprised at all, as that was always the likely outcome.</p>

<p><img src="/img/free_energy1/media/image1.png" alt="graph of -lnP(x)" /></p>

<p>Let’s drill a little deeper into this business of sensing something and - on the basis of that sensation - forming accurate beliefs about the temperature of the environment and your body. It’s worth saying: you don’t have a nice digital-thermometer organ attached somewhere to your body which your brain can just look at. You have millions of tiny sensory receptors, which fire because of the energetic bumping and jostling of atoms hitting the receptor. For a temperature receptor, when it’s hotter, the atoms hitting it have a higher average energy, which makes it more likely that the neuron the receptor is attached to is <em>depolarised</em> and fires an <strong>action potential</strong> (“spikes”). We can roughly reason that in hotter environments, our temperature receptors are firing more often (but only on average - it’s still a “noisy” signal), and so maybe if our brain counted the number of spikes in a certain time, it could learn a mapping from the state of the sensory data it receives to the probable temperature of the environment.</p>

<p><strong>This gives us our third big clue about the FEP: we don’t directly experience the environment, only the noisy sensations that correlate with it.</strong> This is really the jumping off point for the FEP: as an organism, we need to minimise our surprisal (we don’t want to find ourselves in 800 degree heat, and do want to find ourselves at 37 degrees, with high probability), but we only have access to our noisy sense data, and we don’t know what causally determines our temperature in the environment.</p>

<h2 id="getting-bayesian">Getting Bayesian</h2>

<p>As organisms, we want to update our beliefs about the true state of the environment, given some sense data as evidence. That’s right, it’s time for<sup id="fnref:8" role="doc-noteref"><a href="#fn:8" class="footnote">8</a></sup></p>

<p><img src="/img/free_energy1/media/image2.png" alt="Bayesian Inference!!" /></p>

<p><em>(obligatory link to <a href="https://yudkowsky.net/rational/bayes">Yudkowsky on Bayes Theorem</a>. If this is new, read this now)</em></p>

<p>Bayes theorem tells us the optimal way to update our beliefs given some new evidence. Doing this is sometimes called Bayesian inference, or Bayesian updating, or - controversially - as “not making up your beliefs”.</p>

<p><img src="/img/free_energy1/media/bayes.png" alt="Bayes Theorem" /></p>

<p>What we want is to update our belief about the state of the environmental temperature, given our current sensory data:</p>

<p>[Let\ P(Temp = 37) \equiv P(T)] [Let\ P(Sensation = 40\ spikes\ ) \equiv P(S)]</p>

<table>
  <tbody>
    <tr>
      <td>[P(T=37</td>
      <td>S=40)]</td>
    </tr>
  </tbody>
</table>

<p>So our hypothesis is that the temperature is 37 degrees, and the evidence we’re using to evaluate it is the 40 spikes we received in the last few seconds.</p>

<p>Now if we assume that our environmental states are a continuous variable (temperature is just a real number) then we can make the denominator into an integral over the environmental states (possible temperatures):</p>

<table>
  <tbody>
    <tr>
      <td>[P\left( T \middle</td>
      <td>S \right) = \frac{P\left( S \middle</td>
      <td>T \right)P\left( T \right)}{\int_{}^{}{P\left( S \middle</td>
      <td>T \right)P\left( T \right)\text{dT}}}]</td>
    </tr>
  </tbody>
</table>

<p>And there you have it. All you need to do to perfectly understand the world is know the probability of experiencing some sensory data given a particular environmental state, and evaluate the probability of experiencing that sensory data under every possible hypothetical temperature. Very simple stuff!</p>

<p>Okay, so that integral is intractable, which is formal mathematical language for ‘Wolfram Alpha can’t compute it’. This is a problem for the FEP, and much of the rest of our effort will go into getting around that integral. When we can’t get an exact result, we turn to approximate methods. We’re going to manipulate our equation above until it’s in a form that allows us to do <strong>approximate Bayesian inference</strong>.</p>

<h2 id="approximating-your-posterior">Approximating your posterior</h2>

<table>
  <tbody>
    <tr>
      <td>We want to find the <strong>posterior probability</strong> (the probability we get after applying a Bayesian update) that: [P(T=37</td>
      <td>S)]</td>
    </tr>
  </tbody>
</table>

<p>It would really help us control our body temperature if we understood the causal influences determining it. Intuitively, we want a world-model that predicts that if we move closer to a heat-source, we’ll get sensory data that tells us we’re getting hotter.</p>

<p>The internal model that encodes how we expect our sense data to correlate with our environmental states is called a <strong>Generative Model</strong>, and is referred to by Friston as the G-density. Our G-density tells us the joint probability of experiencing some sense data and a corresponding environmental state: [P(T,S)].</p>

<p>You can imagine this as a big table that assigns a probability to each possible pair of values. A good model in this case would assign a high probability to the combination:
[P(high\ temp, lots\ of\ spikes)]
and a low probability to things like:
[P(high\ temp, few\ spikes)]</p>

<p>We will also want a function that represents our current ‘best-guess’ as to the causes of our sensory input, which we’ll call the R-density (for Recognition): [q(T)]</p>

<p>This is just a probability distribution over the state of the environment, and we’re using <strong><em>q</em></strong> instead of <strong><em>p</em></strong> to remind us that it’s a distribution we’re guessing at.</p>

<p>We now need another fancy piece of information theory: the <strong>Kullback-Liebler (KL) Divergence</strong><sup id="fnref:9" role="doc-noteref"><a href="#fn:9" class="footnote">9</a></sup></p>

<p><strong>The KL Divergence just tells us how different two different probability distributions are:</strong></p>

<p><img src="/img/free_energy1/media/image3.png" alt="KL-divergence" /></p>

<p>We want the KL divergence because we want to know how close or far away our best guess is to the true posterior belief <em>if we could compute that ugly integral</em></p>

<p>We write the KL-divergence between our guess <em>q(T)</em> about the environment and the optimal posterior belief about the environment from Baye’s theorem.</p>

<table>
  <tbody>
    <tr>
      <td>[D_{\text{KL}}(q(T)</td>
      <td> </td>
      <td>P(T</td>
      <td>S))\  = \ \int_{}^{}{\text{dT}\ q( T )\ln\frac{q\left( T \right)}{P\left( T \middle</td>
      <td>S \right)}}]</td>
    </tr>
  </tbody>
</table>

<p>which we rearrange using the property of the logarithm to:</p>

<table>
  <tbody>
    <tr>
      <td>[= \ \int_{}^{}{dT\ \lbrack q\left(T \right)\ln{q(T)\  - \ P(T</td>
      <td>S)\rbrack}}]</td>
    </tr>
  </tbody>
</table>

<p>Now, any R-density (that is, any <em>q(T)</em>) which minimises this KL divergence, must be a good approximation of our true posterior. The only problem is that we don’t know the true posterior (that’s the thing we’re trying to work out in the first place), and so can’t simply guess a <em>q(T)</em> to see if it minimises the KL Divergence, because we don’t know what we’re comparing it to.</p>

<table>
  <tbody>
    <tr>
      <td>To get around this, we’ll rewrite the true posterior **P(T</td>
      <td>S)** using some pretty basic probability theory identities:</td>
    </tr>
  </tbody>
</table>

<table>
  <tbody>
    <tr>
      <td>[P\left( T \middle</td>
      <td>S \right) = \frac{P\left( S \middle</td>
      <td>T \right)P\left( T \right)}{P\left( S \right)}]</td>
    </tr>
  </tbody>
</table>

<table>
  <tbody>
    <tr>
      <td>[P\left( T,S \right) = P\left( S \middle</td>
      <td>T \right)P\left( T \right)]</td>
    </tr>
  </tbody>
</table>

<table>
  <tbody>
    <tr>
      <td>[\therefore P\left( T \middle</td>
      <td>S \right) = \frac{P\left( T,S \right)}{P\left( S \right)}]</td>
    </tr>
  </tbody>
</table>

<p>If we take the natural log of both sides:</p>

<table>
  <tbody>
    <tr>
      <td>[\ln{P \left( T \middle</td>
      <td>S \right)} = \ln{\frac{P\left( T,S \right)}{P\left( S \right)} = \ln{P\left( T,S \right)} - \ln{P \left( S \right)}}]</td>
    </tr>
  </tbody>
</table>

<table>
  <tbody>
    <tr>
      <td>Plugging this expression for **P(T</td>
      <td>S)** into our KL Divergence we get:</td>
    </tr>
  </tbody>
</table>

<table>
  <tbody>
    <tr>
      <td>[D_{\text{KL}}(q(T)</td>
      <td> </td>
      <td>P(T</td>
      <td>S))\  = \int_{}^{}{dT\ \lbrack q(T)\ \ln{q\left( T \right) - \ln{P\left( T,S \right) + \ln{P\left( S \right)}}}}\rbrack]</td>
    </tr>
  </tbody>
</table>

<p>Combining the first two <em>ln</em> terms:</p>

<table>
  <tbody>
    <tr>
      <td>[D_{\text{KL}}(q(T)</td>
      <td> </td>
      <td>P(T</td>
      <td>S))\  = \int_{}^{}{dT\ \lbrack q(T)\ \ln{\frac{q\left( T \right)}{P\left( T,S \right)}\  + \ \ln{P\left( S \right)}\ }}\rbrack]</td>
    </tr>
  </tbody>
</table>

<p>Something you may notice about this is that we now have our KL divergence in terms of only our <strong>R-density, q(T)</strong>, and our <strong>G-density, P(T,S)</strong>, plus a <strong>‘surprisal’ term lnP(S)</strong> which - as we discussed above - just tells us how unexpected some sense data is. This looks like progress!</p>

<p>We can pull the <strong>lnP(S)</strong> out from under the integral because we have the requirement that:</p>

<p>[\int_{}^{}{\text{dT} q \left( T \right) = 1}]</p>

<p>This is just the requirement that probabilities sum to one. Integration is linear, so we have:</p>

<table>
  <tbody>
    <tr>
      <td>[D_{\text{KL}}(q(T)</td>
      <td> </td>
      <td>P(T</td>
      <td>S))\  = \int_{}^{}{\text{dT}\ q(T)\ \ln{\frac{q\left( T \right)}{P\left( T,S \right)}\ }} + \int_{}^{}{dT\ q(T)\ \ln{P(S)}}]</td>
    </tr>
  </tbody>
</table>

<table>
  <tbody>
    <tr>
      <td>[D_{\text{KL}}(q(T)</td>
      <td> </td>
      <td>P(T</td>
      <td>S))\  = \int_{}^{} \text{dT}\left\lbrack q \left( T \right)\ln{\frac{q\left( T \right)}{P\left( T,S \right)}\ } \right\rbrack + \ln{P\left( S \right) \times 1}]</td>
    </tr>
  </tbody>
</table>

<p>Now, we define:</p>

<p>[F\  \equiv \int_{}^{}\text{dT}\left\lbrack q\left( T \right)\ln{\frac{q\left( T \right)}{P\left( T,S \right)}\ } \right\rbrack]</p>

<p>Giving us:</p>

<table>
  <tbody>
    <tr>
      <td>[D_{\text{KL}}(q(T)</td>
      <td> </td>
      <td>P\left( T \middle</td>
      <td>S \right)) = F + \ln{P(S})]</td>
    </tr>
  </tbody>
</table>

<p>An important property of the KL Divergence is that it is always greater than or equal to zero. The formula above tells us that if our KL divergence went to zero (that is, if our R-density became a perfect approximation of our true posterior), we would have:</p>

<p>[0 = F + \ln{P\left( S \right)}]</p>

<p>[F = - \ln{P(S)}]</p>

<p>If you haven’t guessed already, the F we defined above is ‘free energy’. Now we have a key result in the Free Energy literature, one which Friston refers to all the time:</p>

<h3 id="free-energy-is-an-upper-bound-on-surprisal">Free energy is an upper bound on surprisal!</h3>

<p>We can see this because we know that the KL divergence is always greater than or equal to zero, implying:</p>

<table>
  <tbody>
    <tr>
      <td>[D_{\text{KL}}(q(T)</td>
      <td> </td>
      <td>P\left( T \middle</td>
      <td>S \right)) \geq 0]</td>
    </tr>
  </tbody>
</table>

<p>[\Rightarrow F\  + \ \ln{P\left( S \right)} \geq \ 0]</p>

<p>[F\  \geq \  - \ln{P(S)}]</p>

<p>More specifically, <strong>free energy is the surprisal an organism experiences upon sampling some data, given a generative model.</strong> If you’re up for it, you should try matching those words to the parts of the equations that encode them!</p>

<table>
  <tbody>
    <tr>
      <td>So far, we have found that this quantity ‘free energy’ is an upper bound on surprisal, and we notice that minimising it means we are approximating the true posterior, **P(E</td>
      <td>S)**!</td>
    </tr>
  </tbody>
</table>

<h2 id="another-form-of-the-free-energy">Another form of the free energy:</h2>

<p>We’re going to need to massage this equation for F a bit more to see if something useful pops out.</p>

<p>Again, using the linearity of integration, and the properties of the logarithm, we have:</p>

<p>[F\  \equiv \int_{}^{}\text{dT}\left\lbrack \ q\left( T \right)\ln{\frac{q\left( T \right)}{P\left( T,S \right)}\ } \right\rbrack]</p>

<p>[F\  = \ \int_{}^{}\text{dT}{\ q(T)\ \ln{q(T)\ \ } -}\int_{}^{}{\text{dT}\ q(T)\ln{P(T,S)}}]</p>

<p>From statistical mechanics, we say that the expected value (or average) energy of a system is simply the sum over all energy states, times the probability of each energy state:</p>

<p>[E\lbrack X\rbrack\  = \ \sum_{i}^{}{X_{i}P(X_{i})}]</p>

<p>And in the continuous case:</p>

<p>[\displaystyle \operatorname {E} [X]=\int _{\mathbb {R} }xP(x)\,dx]</p>

<p>We also have the entropy of a system (precisely the average surprise in the probability distribution) as:</p>

<p>[\displaystyle \mathrm {H} (X)=-\sum <em>{i=1}^{n}{\mathrm {P} (x</em>{i})\log \mathrm {P} (x_{i}})]</p>

<p><img src="/img/free_energy1/media/image4.png" alt="A close up of a device Description automatically generated" /></p>

<p>Entropy can be a somewhat tricky term, but I think this way of thinking about it is fairly intuitive: it’s just the amount you expect to be surprised by a given probability distribution. Some distributions are very tightly clustered around their average values, and so they are very unsurprising, hence low entropy. The opposite of this are the so-called maximum-entropy distributions, which means every sample is maximally surprising.</p>

<p>Equipped with these ideas, we define a function <strong>E(T,S)</strong>:</p>

<p>[\mathbf{Ε}(T,S)\  \equiv - \ln{P(T,S)}]</p>

<p>And this allows us to rewrite our free energy term F as:</p>

<p>[F\  = \ \int_{}^{}{\text{dT}\ q(T)\ \ln{q(T)\ \ } -}\int_{}^{}{\text{dT}\ q(T)\ln{P(T,S)}}]</p>

<p>[F = \int_{}^{}{\text{dT}\ q\left( T \right)\mathbf{E}\left( T,S \right)} + \int_{}^{}{\text{dT}\ q\left( T \right)\ln{q\left( T \right)}} ]</p>

<p>[F = \int_{}^{}{\text{dT}\ q\left( T \right) \mathbf{E}\left( T,S \right) } - \left( - \int_{}^{}{\text{dT}\ q \left( T \right)\ln{q\left( T \right)} } \right)]</p>

<p>Which (check that you see this from above) looks like it’s saying that ‘free energy’ is equal to an average energy, minus something that looks a little like a continuous version of the entropy<sup id="fnref:10" role="doc-noteref"><a href="#fn:10" class="footnote">10</a></sup>. This version of the formula is something you’ll hear Friston refer to often, because it’s analagous to the  <strong>Helmholtz free energy</strong> from thermodynamics/statistical mechanics. The Helmholtz free energy is defined as the difference between the internal energy and the entropy of the system (multiplied by the temperature, but ignore that here). Here the term ‘free energy’ acquires some physical sense, being the quantity of energy in our system that is available to do useful work.</p>

<!-- \[\displaystyle F\equiv U-TS,\] -->

<p>In the next post, we’ll take the background we developed here and build on it. We’ll take a deeper look at the R and G densities and some simplifying assumptions that allow us to write neat versions of them. The result will show the deep connection between the Free Energy Principle and Predictive Coding in the brain.</p>

<p>If something here doesn’t make sense, is clearly wrong, or got you interested, let me know here or on twitter <a href="https://twitter.com/jnearestn">@jnearestn</a>
If you don’t want to wait for the next post, <a href="https://arxiv.org/abs/1705.09156">The Free Energy Principle for Action and Perception: A Mathematical Review</a> is the best place to start out on your own, and I owe it a great deal in helping write this post!</p>

<p>Cover image: Andrestand on <a href="https://www.flickr.com/photos/andrestand/6703933473/in/photolist-bdppRT-chFXkA-ZXYEXs-ZXYEnj-ZXYCk3-ZZV2jS-ZXYGCw-YY2eRL-YY2ahL-YY26ks-HpMxzn-213CToV-ZZVhPj-YY2hmW-ZZVazQ-G6ZRTH-213CU54-213CPWp-ZXYbGE-213CPqe-ZZVc3j-ZZUYEd-YY1MNU-2117VfH-213CRsv-CWzsmh-YY2i8A-ZXYcfJ-ZZUXvj-ZZUG9Y-CWzord-YY2hJE-ZZVfdN-YY2czS-YY29XN-YY26Ws-ZXnhRu-G4un7K-ZZV1Gu-YY1PxL-ZXkmRs-CWzuDy-ZZVdW9-ZXYtJ5-ZXYcQw-YVqA6W-CWzs1s-ykSEWf-YY2b7w-G6ZWXk">Flickr</a></p>
<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>The good news is that the Jeremy Howard rule is holds: even if all of the math seems difficult, it will look much simpler in code <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:2" role="doc-endnote">
      <p>shoutout to the New York Times for going full 2020 and trying to <a href="https://slatestarcodex.com/2020/06/22/nyt-is-threatening-my-safety-by-revealing-my-real-name-so-i-am-deleting-the-blog/">ruin that too</a> <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:3" role="doc-endnote">
      <p>See for example Friston’s reaction to a bunch of quants in <a href="https://www.wired.com/story/karl-friston-free-energy-principle-artificial-intelligence/">this article</a> <a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:4" role="doc-endnote">
      <p>in fact, this multiplicity of disciplines seems to set off a percentage of people’s BS-detectors, sort of like when Deepak Chopra starts invoking Quantum Mechanics as an explanation for everything <a href="#fnref:4" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:5" role="doc-endnote">
      <p>Fully realised enlightened Buddhists with no sense-of-self don’t @ me yet please, it’s just a starting point <a href="#fnref:5" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:6" role="doc-endnote">
      <p>if you don’t like 37, just use whatever number you were told by the last person who pointed one of those COVID-screening-thermometer-guns at your head <a href="#fnref:6" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:7" role="doc-endnote">
      <p>there is of course the question of ‘why expect 37 degrees in the first place, why not 800 and then you can survive through more possible states’, to which the quick answer is “evolution” and evolutionary niches” – something like: that temperature is the constraint that you, as a particular organism, are forced to satisfy because of the particular set of biochemical pathways you evolved to best fit into your environmental niche (which seems, at this point, to be Zoom Meetings, for some reason?) <a href="#fnref:7" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:8" role="doc-endnote">
      <p>9 year old Jared knew a killer font when he saw one <a href="#fnref:8" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:9" role="doc-endnote">
      <p>watch <a href="https://www.youtube.com/watch?v=ErfnhcEV1O8">this video by Aurelien Geron</a> if you want a great intro to the KL-divergence <a href="#fnref:9" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:10" role="doc-endnote">
      <p>as I’m writing this, I’m learning that this continuous analogy of the entropy is not actually well-defined. It’s called differential entropy, and Claude Shannon apparently just wrote it down, assuming it was correct (okay, now I feel less bad for making the same assumption). It took E.T Jaynes to write down a better version called the ‘Limiting Density of Discrete Points’, which - at minimum - is a worse name than ‘differential entropy’. I don’t know what effect the ill-definedness of continuous entropy has for the FEP, so that’s something to look into while I write part 2! <a href="#fnref:10" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>

  </div><a class="u-url" href="/blog/2020/08/08/free-energy1.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/blog/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/blog/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/blog/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>so it goes</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/jaredtumiel" title="jaredtumiel"><svg class="svg-icon grey"><use xlink:href="/blog/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/jnearestn" title="jnearestn"><svg class="svg-icon grey"><use xlink:href="/blog/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
